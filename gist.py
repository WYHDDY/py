import requests
from lxml import html
import time
from urllib.parse import quote_plus

PROXIES = None

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
}


def fetch_page_content(url, session):
    try:
        response = session.get(url, headers=HEADERS, proxies=PROXIES, timeout=20)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚é”™è¯¯: {url} - {e}")
        return None

def run_scraper(search_terms):
    print("--- å¯åŠ¨ Gist çˆ¬è™«æ¨¡å¼ ---")
    
    start_page = 1
    end_page = 3
    print(f"â„¹ï¸ å·²å›ºå®šçˆ¬å–é¡µæ•°èŒƒå›´ä¸º: {start_page} åˆ° {end_page}")

    print("â„¹ï¸ æœªè®¾ç½®ä»£ç†ï¼Œå°†ç›´æ¥è¿æ¥ã€‚")

    all_final_urls = []

    with requests.Session() as session:
        for search_query in search_terms:
            print(f"\n--- å¼€å§‹æœç´¢å†…å®¹: '{search_query}' ---")
            encoded_query = quote_plus(search_query)

            for page_num in range(start_page, end_page + 1):
                search_url = f"https://gist.github.com/search?l=YAML&o=desc&p={page_num}&q={encoded_query}&s=updated"
                print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ: {search_url}")

                html_content = fetch_page_content(search_url, session)
                if not html_content:
                    continue

                tree = html.fromstring(html_content)
                gist_snippets = tree.xpath("//div[@class='gist-snippet']")

                if not gist_snippets:
                    print(f"Â  Â  -> åœ¨ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°ä»»ä½• Gist ç»“æœã€‚")
                    break
                
                print(f"Â  Â  -> åœ¨ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(gist_snippets)} ä¸ª Gist ç»“æœã€‚")

                for index, snippet in enumerate(gist_snippets):
                    partial_links = snippet.xpath(".//div[contains(@class, 'gist-snippet-meta')]//a[1]")
                    
                    if not partial_links: continue
                    partial_gist_href = partial_links[0].get('href')
                    if not partial_gist_href: continue

                    full_gist_url = f"https://gist.github.com{partial_gist_href}"
                    
                    print(f"Â  Â  Â  [{index + 1}/{len(gist_snippets)}] æ­£åœ¨è®¿é—® Gist é¡µé¢: {full_gist_url}")
                    gist_page_content = fetch_page_content(full_gist_url, session)
                    if not gist_page_content: continue

                    gist_tree = html.fromstring(gist_page_content)
                    raw_link_elements = gist_tree.xpath("//a[contains(., 'Raw')]")

                    if raw_link_elements:
                        raw_file_href = raw_link_elements[0].get('href')
                        final_url = f"https://gist.github.com{raw_file_href}"
                        
                        if final_url.endswith('all.yaml'):
                            print(f"Â  Â  Â  âœ… æˆåŠŸæå–: {final_url}")
                            all_final_urls.append(final_url)
                        else:
                            print(f"Â  Â  Â  â¡ï¸ é“¾æ¥ä¸ç¬¦åˆè¦æ±‚ï¼Œå·²è·³è¿‡: {final_url}")
                    else:
                        print(f"Â  Â  Â  âš ï¸ åœ¨ Gist é¡µé¢ {full_gist_url} æœªæ‰¾åˆ° 'Raw' é“¾æ¥ã€‚")
                    
                    time.sleep(0.5)

    if all_final_urls:
        unique_urls = sorted(list(set(all_final_urls)))
        count = len(unique_urls)
        filename = "gist.txt"
        
        with open(filename, 'w', encoding='utf-8') as f:
            for url in unique_urls:
                f.write(f"{url}\n")
        
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆ! å…±æå– {count} ä¸ªå»é‡åçš„ URLï¼Œå·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
    else:
        print("\nğŸ¤·â€ ä»»åŠ¡ç»“æŸï¼Œæ²¡æœ‰æå–åˆ°ä»»ä½• URLã€‚")

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    fixed_search_terms = ['ss', 'ssr', 'vless', 'vmess', 'trojan']
    run_scraper(fixed_search_terms)
